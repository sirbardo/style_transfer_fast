{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    shortcut = y\n",
    "\n",
    "    # down-sampling is performed with a stride of 2\n",
    "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU()(y)\n",
    "\n",
    "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = layers.Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    y = layers.add([shortcut, y])\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Deconv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "# a little Demo\n",
    "#inputs = Input((img_rows, img_cols, num_channels))\n",
    "#padded_inputs = ReflectionPadding2D(padding=(1,1))(inputs)\n",
    "#conv1 = Conv2D(32, 3, padding='valid', kernel_initializer='he_uniform',\n",
    "#               data_format='channels_last')(padded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell I define, functionally, the \"image transformation\" network from jc johnson's and li fei fei paper.\n",
    "\"\"\"\n",
    "\n",
    "inputs = Input(shape=[256, 256, 3])\n",
    "#o = ReflectionPadding2D(padding=(40,40))(inputs) #TODO: Check if the reflection padding is done as the paper\n",
    "o = Conv2D(32, (9, 9), strides=(1,1), padding='same', activation='relu')(inputs)\n",
    "o = Conv2D(64, (3, 3), strides=(2,2), padding='same', activation='relu')(o)\n",
    "o = Conv2D(128, (3, 3), strides=(2,2), padding='same', activation='relu')(o)\n",
    "o = residual_block(o, 128)\n",
    "o = residual_block(o, 128)\n",
    "o = residual_block(o, 128)\n",
    "o = residual_block(o, 128) #TODO: check if the residual blocks are the same as the paper\n",
    "o = residual_block(o, 128)\n",
    "o = Deconv2D(64, (3, 3), strides=2, padding='same', activation='relu')(o)\n",
    "o = Deconv2D(32, (3, 3), strides=2, padding='same', activation='relu')(o) #TODO: check if the output of this layer is the same as the fractional stride conv\n",
    "o = Conv2D(3, (9, 9), strides=1, activation='tanh', padding='same')(o)\n",
    "\n",
    "model_style_transfer = Model(inputs=inputs, outputs=o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.layers # let's see where to get our outputs for style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu1_2 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=2).output])\n",
    "relu2_2 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=5).output])\n",
    "relu3_3 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=9).output])\n",
    "relu4_3 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=13).output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"fortnite/34.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provaoutput = relu4_3.predict(np.array([im]))[0,:,:,43]\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(provaoutput, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_objective(y_true, y_pred):\n",
    "    '''Just another crossentropy'''\n",
    "    y_true = tf.Variable(trainable=False, initial_value=tf.float32(0.0))\n",
    "    distance = y_pred - y_true\n",
    "    return distance\n",
    "\n",
    "def gram_matrix_pre(x):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        features = x.flatten()\n",
    "    else:\n",
    "        features = np.transpose(x, (2, 0, 1)).flatten()\n",
    "\n",
    "    shape = np.shape(x)\n",
    "    \n",
    "    C, W, H = (shape[0],shape[1], shape[2])\n",
    "    \n",
    "    cf = np.reshape(features ,(C,-1))\n",
    "    gram = np.dot(cf, np.transpose(cf)) /  np.float32(C*W*H)\n",
    "\n",
    "    return gram\n",
    "\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "\n",
    "    shape = K.shape(x)\n",
    "    \n",
    "    C, W, H = (shape[0],shape[1], shape[2])\n",
    "    \n",
    "    cf = K.reshape(features ,(C,-1))\n",
    "    gram = K.dot(cf, K.transpose(cf)) /  K.cast(C*W*H,dtype='float32')\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.resize(im, (256, 256))\n",
    "style_gram_1 = gram_matrix_pre(relu1_2.predict(np.array([im]))[0])\n",
    "style_gram_2 = gram_matrix_pre(relu2_2.predict(np.array([im]))[0])\n",
    "style_gram_3 = gram_matrix_pre(relu3_3.predict(np.array([im]))[0])\n",
    "style_gram_4 = gram_matrix_pre(relu4_3.predict(np.array([im]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/datasets/'#/coco-2014-train/'\n",
    "path_actual = '/datasets/coco-2014-train/'\n",
    "X = [cv2.imread(path_actual+name) for name in os.listdir(path_actual)[:50]]\n",
    "X = [cv2.resize(item, (256, 256)) for item in X]\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer\n",
    "\n",
    "def pre_loss(inputs_to_use):\n",
    "    def loss_function_for_style_transfer(y_true, y_pred):\n",
    "\n",
    "\n",
    "        #relu1_2 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=2).output])\n",
    "        #relu2_2 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=5).output])\n",
    "        #relu3_3 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=9).output])\n",
    "        #relu4_3 = Model(inputs=[vgg.get_layer(index=0).input], outputs=[vgg.get_layer(index=13).output])\n",
    "        inputs1 = Input(tensor=y_pred)\n",
    "        model_loss = inputs1\n",
    "        for i in range (1,14):\n",
    "            model_loss = vgg.layers[i](model_loss)\n",
    "\n",
    "        model_loss_final = Model(inputs=[inputs1], outputs=[model_loss])\n",
    "        \n",
    "    \n",
    "        loss_net_relu1_2 = model_loss_final.get_layer(index=2).get_output_at(1)\n",
    "        loss_net_relu2_2 = model_loss_final.get_layer(index=5).get_output_at(1)\n",
    "        loss_net_relu3_3 = model_loss_final.get_layer(index=9).get_output_at(1)\n",
    "        loss_net_relu4_3 = model_loss_final.get_layer(index=13).get_output_at(1)\n",
    "        \n",
    "        model_feature = inputs_to_use\n",
    "        for i in range (1,10):\n",
    "            model_feature = vgg.layers[i](model_feature)\n",
    "\n",
    "\n",
    "        loss_net_feature_relu3_3 = Model(inputs=[inputs_to_use], outputs=[model_feature]).output\n",
    "\n",
    "        gram1_style = K.variable(style_gram_1)\n",
    "        gram2_style = K.variable(style_gram_2)\n",
    "        gram3_style = K.variable(style_gram_3)\n",
    "        gram4_style = K.variable(style_gram_4)\n",
    "\n",
    "        gram_1 = gram_matrix(loss_net_relu1_2[0])\n",
    "        gram_2 = gram_matrix(loss_net_relu2_2[0])\n",
    "        gram_3 = gram_matrix(loss_net_relu3_3[0])\n",
    "        gram_4 = gram_matrix(loss_net_relu4_3[0])\n",
    "\n",
    "\n",
    "        style1_loss = K.sum(K.square(K.abs(gram1_style - gram_1)))\n",
    "        style2_loss = K.sum(K.square(K.abs(gram2_style - gram_2)))\n",
    "        style3_loss = K.sum(K.square(K.abs(gram3_style - gram_3)))\n",
    "        style4_loss = K.sum(K.square(K.abs(gram4_style - gram_4)))\n",
    "\n",
    "        print(loss_net_feature_relu3_3.shape)\n",
    "        print(loss_net_relu3_3.shape)\n",
    "        feature_loss = K.mean(K.square(loss_net_feature_relu3_3 - loss_net_relu3_3))\n",
    "\n",
    "        return feature_loss+style1_loss+style2_loss+style3_loss+style4_loss\n",
    "\n",
    "    return loss_function_for_style_transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model_style_transfer.compile('nadam', loss=[pre_loss(model_style_transfer.input)])\n",
    "\n",
    "#model_style_transfer.fit(X, X, batch_size=1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=1,\n",
    "        class_mode='input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1)\n",
    "tensorboard = TensorBoard(batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1994/86000 [..............................] - ETA: 1:00:05 - loss: 8919469106.8405"
     ]
    }
   ],
   "source": [
    "model_style_transfer.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=86000,\n",
    "        epochs=5,\n",
    "        callbacks=[checkpointer, tensorboard]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_style_transfer.predict(X[:2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
